{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier, RandomForestClassifier,\n",
    "    GBTClassifier, LinearSVC, NaiveBayes, MultilayerPerceptronClassifier,\n",
    "    FMClassifier)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, calibration_curve\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular métricas\n",
    "def evaluate_model(model, train_df, test_df, variavel_resposta):\n",
    "    fitted_model = model.fit(train_df)\n",
    "    predictions = fitted_model.transform(test_df)\n",
    "    \n",
    "    metrics = {}\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=variavel_resposta, predictionCol=\"prediction\")\n",
    "    for metric in [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]:\n",
    "        metrics[metric] = evaluator.evaluate(predictions, {evaluator.metricName: metric})\n",
    "    \n",
    "    if len(predictions.select(variavel_resposta).distinct().collect()) == 2:\n",
    "        auc_evaluator = BinaryClassificationEvaluator(labelCol=variavel_resposta, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        metrics[\"AUC\"] = auc_evaluator.evaluate(predictions)\n",
    "        metrics[\"KS\"] = calculate_ks(predictions, variavel_resposta)\n",
    "    \n",
    "    metrics[\"ConfusionMatrix\"] = predictions.groupBy(variavel_resposta, \"prediction\").count().orderBy(variavel_resposta, \"prediction\").collect()\n",
    "    return metrics, predictions\n",
    "\n",
    "# Função para calcular o KS\n",
    "def calculate_ks(predictions, variavel_resposta):\n",
    "    sorted_predictions = predictions.orderBy(F.desc(\"probability\"))\n",
    "    tpr = sorted_predictions.withColumn(\"tpr\", F.sum(variavel_resposta).over(Window.orderBy(F.desc(\"probability\"))))\n",
    "    fpr = sorted_predictions.withColumn(\"fpr\", F.sum(1 - F.col(variavel_resposta)).over(Window.orderBy(F.desc(\"probability\"))))\n",
    "    ks = tpr.withColumn(\"ks\", F.col(\"tpr\") - F.col(\"fpr\")).agg(F.max(\"ks\")).collect()[0][0]\n",
    "    return ks\n",
    "\n",
    "# Função para salvar o modelo e os hiperparâmetros\n",
    "def save_model_and_params(model, params, model_name, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}_model\")\n",
    "    model.save(model_path)\n",
    "    params_path = os.path.join(save_dir, f\"{model_name}_params.yaml\")\n",
    "    with open(params_path, 'w') as file:\n",
    "        yaml.dump(params, file)\n",
    "    print(f\"Modelo e hiperparâmetros salvos em: {save_dir}\")\n",
    "\n",
    "# Função para plotar a matriz de confusão\n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    labels = sorted(set([row[0] for row in confusion_matrix] + [row[1] for row in confusion_matrix]))\n",
    "    matrix = np.zeros((len(labels), len(labels)))\n",
    "    for row in confusion_matrix:\n",
    "        matrix[labels.index(row[0])][labels.index(row[1])] = row[2]\n",
    "    fig = px.imshow(matrix, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), x=labels, y=labels, text_auto=True, color_continuous_scale=\"Blues\")\n",
    "    fig.update_layout(title=\"Confusion Matrix\", xaxis_title=\"Predicted\", yaxis_title=\"Actual\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para plotar a curva ROC\n",
    "def plot_roc_curve(predictions, variavel_resposta):\n",
    "    pdf = predictions.select(\"probability\", variavel_resposta).toPandas()\n",
    "    prob = np.array([p[1] for p in pdf[\"probability\"]])\n",
    "    true_labels = pdf[variavel_resposta]\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode=\"lines\", name=f\"ROC curve (area = {roc_auc:.2f})\", line=dict(color=\"darkorange\", width=2)))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"Random (area = 0.5)\", line=dict(color=\"navy\", width=2, dash=\"dash\")))\n",
    "    fig.update_layout(title=\"Receiver Operating Characteristic (ROC)\", xaxis_title=\"False Positive Rate\", yaxis_title=\"True Positive Rate\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para plotar a curva Precision-Recall\n",
    "def plot_pr_curve(predictions, variavel_resposta):\n",
    "    pdf = predictions.select(\"probability\", variavel_resposta).toPandas()\n",
    "    prob = np.array([p[1] for p in pdf[\"probability\"]])\n",
    "    true_labels = pdf[variavel_resposta]\n",
    "    precision, recall, thresholds = precision_recall_curve(true_labels, prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=recall, y=precision, mode=\"lines\", name=f\"PR curve (area = {pr_auc:.2f})\", line=dict(color=\"blue\", width=2)))\n",
    "    fig.update_layout(title=\"Precision-Recall Curve\", xaxis_title=\"Recall\", yaxis_title=\"Precision\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para plotar a distribuição de probabilidades\n",
    "def plot_probability_distribution(predictions, variavel_resposta):\n",
    "    pdf = predictions.select(\"probability\", variavel_resposta).toPandas()\n",
    "    prob = np.array([p[1] for p in pdf[\"probability\"]])\n",
    "    true_labels = pdf[variavel_resposta]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=prob[true_labels == 1], name=\"Classe Positiva\", opacity=0.75, marker_color=\"green\"))\n",
    "    fig.add_trace(go.Histogram(x=prob[true_labels == 0], name=\"Classe Negativa\", opacity=0.75, marker_color=\"red\"))\n",
    "    fig.update_layout(title=\"Distribuição de Probabilidades\", xaxis_title=\"Probabilidade da Classe Positiva\", yaxis_title=\"Frequência\", barmode=\"overlay\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para plotar a importância das features\n",
    "def plot_feature_importance(model, feature_names):\n",
    "    if hasattr(model, \"featureImportances\"):\n",
    "        importances = model.featureImportances.toArray()\n",
    "        importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances}).sort_values(by=\"Importance\", ascending=False)\n",
    "        fig = px.bar(importance_df, x=\"Feature\", y=\"Importance\", title=\"Importância das Features\")\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"Este modelo não suporta importância de features.\")\n",
    "\n",
    "# Função para plotar a curva de calibração\n",
    "def plot_calibration_curve(predictions, variavel_resposta):\n",
    "    pdf = predictions.select(\"probability\", variavel_resposta).toPandas()\n",
    "    prob = np.array([p[1] for p in pdf[\"probability\"]])\n",
    "    true_labels = pdf[variavel_resposta]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(true_labels, prob, n_bins=10)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=mean_predicted_value, y=fraction_of_positives, mode=\"lines+markers\", name=\"Curva de Calibração\", line=dict(color=\"blue\", width=2)))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"Ideal\", line=dict(color=\"red\", width=2, dash=\"dash\")))\n",
    "    fig.update_layout(title=\"Curva de Calibração\", xaxis_title=\"Probabilidade Média Prevista\", yaxis_title=\"Fração de Positivos Reais\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para plotar a curva KS\n",
    "def plot_ks_curve(predictions, variavel_resposta):\n",
    "    sorted_predictions = predictions.orderBy(F.desc(\"probability\"))\n",
    "    tpr = sorted_predictions.withColumn(\"tpr\", F.sum(variavel_resposta).over(Window.orderBy(F.desc(\"probability\"))))\n",
    "    fpr = sorted_predictions.withColumn(\"fpr\", F.sum(1 - F.col(variavel_resposta)).over(Window.orderBy(F.desc(\"probability\"))))\n",
    "    ks_df = tpr.withColumn(\"ks\", F.col(\"tpr\") - F.col(\"fpr\")).toPandas()\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=ks_df.index, y=ks_df[\"tpr\"], mode=\"lines\", name=\"TPR (True Positive Rate)\", line=dict(color=\"blue\", width=2)))\n",
    "    fig.add_trace(go.Scatter(x=ks_df.index, y=ks_df[\"fpr\"], mode=\"lines\", name=\"FPR (False Positive Rate)\", line=dict(color=\"red\", width=2)))\n",
    "    fig.add_trace(go.Scatter(x=ks_df.index, y=ks_df[\"ks\"], mode=\"lines\", name=\"KS\", line=dict(color=\"green\", width=2)))\n",
    "    fig.update_layout(title=\"Curva KS\", xaxis_title=\"Ranking das Probabilidades\", yaxis_title=\"Taxa\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para plotar a curva de Lift\n",
    "def plot_lift_curve(predictions, variavel_resposta):\n",
    "    sorted_predictions = predictions.orderBy(F.desc(\"probability\"))\n",
    "    lift_df = sorted_predictions.withColumn(\"cumulative_gain\", F.sum(variavel_resposta).over(Window.orderBy(F.desc(\"probability\")))).toPandas()\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=lift_df.index, y=lift_df[\"cumulative_gain\"], mode=\"lines\", name=\"Lift Curve\", line=dict(color=\"blue\", width=2)))\n",
    "    fig.update_layout(title=\"Curva de Lift\", xaxis_title=\"Ranking das Probabilidades\", yaxis_title=\"Ganho Cumulativo\")\n",
    "    fig.show()\n",
    "\n",
    "# Função para otimizar hiperparâmetros com Optuna\n",
    "def optimize_model(model_name, train_df, test_df, variavel_resposta, seed):\n",
    "    def objective(trial):\n",
    "            if model_name == \"LogisticRegression\":\n",
    "                model = LogisticRegression(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    regParam=trial.suggest_float(\"regParam\", 0.01, 10.0, log=True),\n",
    "                    elasticNetParam=trial.suggest_float(\"elasticNetParam\", 0.0, 1.0)\n",
    "                )\n",
    "            elif model_name == \"DecisionTreeClassifier\":\n",
    "                model = DecisionTreeClassifier(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    maxDepth=trial.suggest_int(\"maxDepth\", 2, 10),\n",
    "                    minInstancesPerNode=trial.suggest_int(\"minInstancesPerNode\", 1, 10)\n",
    "                )\n",
    "            elif model_name == \"RandomForestClassifier\":\n",
    "                model = RandomForestClassifier(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    numTrees=trial.suggest_int(\"numTrees\", 10, 100),\n",
    "                    maxDepth=trial.suggest_int(\"maxDepth\", 2, 10)\n",
    "                )\n",
    "            elif model_name == \"GBTClassifier\":\n",
    "                model = GBTClassifier(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    maxIter=trial.suggest_int(\"maxIter\", 10, 100),\n",
    "                    maxDepth=trial.suggest_int(\"maxDepth\", 2, 10)\n",
    "                )\n",
    "            elif model_name == \"LinearSVC\":\n",
    "                model = LinearSVC(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    regParam=trial.suggest_float(\"regParam\", 0.01, 10.0, log=True),\n",
    "                    maxIter=trial.suggest_int(\"maxIter\", 10, 100)\n",
    "                )\n",
    "            elif model_name == \"NaiveBayes\":\n",
    "                model = NaiveBayes(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    smoothing=trial.suggest_float(\"smoothing\", 0.0, 10.0)\n",
    "                )\n",
    "            elif model_name == \"MultilayerPerceptronClassifier\":\n",
    "                model = MultilayerPerceptronClassifier(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    layers=[3, trial.suggest_int(\"hiddenLayerSize\", 2, 10), 2],\n",
    "                    maxIter=trial.suggest_int(\"maxIter\", 10, 100)\n",
    "                )\n",
    "            elif model_name == \"FMClassifier\":\n",
    "                model = FMClassifier(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    factorSize=trial.suggest_int(\"factorSize\", 2, 10),\n",
    "                    regParam=trial.suggest_float(\"regParam\", 0.01, 10.0, log=True)\n",
    "                )\n",
    "            elif model_name == \"LightGBMClassifier\":\n",
    "                model = LightGBMClassifier(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    numLeaves=trial.suggest_int(\"numLeaves\", 10, 100),\n",
    "                    maxDepth=trial.suggest_int(\"maxDepth\", 2, 10),\n",
    "                    learningRate=trial.suggest_float(\"learningRate\", 0.01, 0.3, log=True)\n",
    "                )\n",
    "            elif model_name == \"OneVsRest\":\n",
    "                base_model = LogisticRegression(\n",
    "                    featuresCol='features',\n",
    "                    labelCol=variavel_resposta,\n",
    "                    regParam=trial.suggest_float(\"regParam\", 0.01, 10.0, log=True),\n",
    "                    elasticNetParam=trial.suggest_float(\"elasticNetParam\", 0.0, 1.0)\n",
    "                )\n",
    "                model = OneVsRest(classifier=base_model)\n",
    "                \n",
    "                # Treinar e avaliar\n",
    "                fitted_model = model.fit(train_df)\n",
    "                predictions = fitted_model.transform(test_df)\n",
    "                evaluator = MulticlassClassificationEvaluator(labelCol=variavel_resposta, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "                return evaluator.evaluate(predictions)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    return study.best_params\n",
    "\n",
    "# Função principal\n",
    "def main(df, features_cols, variavel_resposta, metricas_disponiveis, metrica_vencedora, seed, save_dir):\n",
    "    spark = SparkSession.builder.appName(\"AutoML\").getOrCreate()\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n",
    "    df = assembler.transform(df)\n",
    "    train_df, test_df = df.randomSplit([0.8, 0.2], seed=seed)\n",
    "    \n",
    "    # Definir todos os modelos\n",
    "    models = [\n",
    "        (\"LogisticRegression\", LogisticRegression(featuresCol='features', labelCol=variavel_resposta)),\n",
    "        (\"DecisionTreeClassifier\", DecisionTreeClassifier(featuresCol='features', labelCol=variavel_resposta, seed=seed)),\n",
    "        (\"RandomForestClassifier\", RandomForestClassifier(featuresCol='features', labelCol=variavel_resposta, seed=seed)),\n",
    "        (\"GBTClassifier\", GBTClassifier(featuresCol='features', labelCol=variavel_resposta, seed=seed)),\n",
    "        (\"LinearSVC\", LinearSVC(featuresCol='features', labelCol=variavel_resposta)),\n",
    "        (\"NaiveBayes\", NaiveBayes(featuresCol='features', labelCol=variavel_resposta)),\n",
    "        (\"MultilayerPerceptronClassifier\", MultilayerPerceptronClassifier(featuresCol='features', \n",
    "                                                                          labelCol=variavel_resposta, \n",
    "                                                                          layers=[len(features_cols), 5, 2], \n",
    "                                                                          seed=seed)),\n",
    "        (\"FMClassifier\", FMClassifier(featuresCol='features', labelCol=variavel_resposta)),\n",
    "        (\"LightGBMClassifier\", LightGBMClassifier(featuresCol='features', labelCol=variavel_resposta, predictionCol=\"prediction\", seed=seed)),\n",
    "        (\"OneVsRest\", OneVsRest(classifier=LogisticRegression(featuresCol='features', labelCol=variavel_resposta)))\n",
    "    ]\n",
    "    \n",
    "    # Avaliar todos os modelos\n",
    "    results = []\n",
    "    for model_name, model in models:\n",
    "        print(f\"Avaliando {model_name}...\")\n",
    "        metrics, predictions = evaluate_model(model, train_df, test_df, variavel_resposta)\n",
    "        results.append((model_name, metrics))\n",
    "        print(f\"{model_name} - Métricas: {metrics}\")\n",
    "    \n",
    "    # Determinar o modelo vencedor com base na métrica escolhida\n",
    "    winning_model_name, winning_metrics = max(results, key=lambda x: x[1][metrica_vencedora])\n",
    "    print(f\"Modelo vencedor: {winning_model_name} com {metrica_vencedora}: {winning_metrics[metrica_vencedora]}\")\n",
    "    \n",
    "    # Otimizar o modelo vencedor\n",
    "    print(f\"Otimizando {winning_model_name} com Optuna...\")\n",
    "    best_params = optimize_model(winning_model_name, train_df, test_df, variavel_resposta, seed)\n",
    "    print(f\"Melhores hiperparâmetros para {winning_model_name}: {best_params}\")\n",
    "    \n",
    "    # Treinar o modelo vencedor com os melhores hiperparâmetros\n",
    "    if winning_model_name == \"LogisticRegression\":\n",
    "        model = LogisticRegression(featuresCol='features', labelCol=variavel_resposta, **best_params)\n",
    "    elif winning_model_name == \"DecisionTreeClassifier\":\n",
    "        model = DecisionTreeClassifier(featuresCol='features', labelCol=variavel_resposta, seed=seed, **best_params)\n",
    "    elif winning_model_name == \"RandomForestClassifier\":\n",
    "        model = RandomForestClassifier(featuresCol='features', labelCol=variavel_resposta, seed=seed, **best_params)\n",
    "    elif winning_model_name == \"GBTClassifier\":\n",
    "        model = GBTClassifier(featuresCol='features', labelCol=variavel_resposta, seed=seed, **best_params)\n",
    "    elif winning_model_name == \"LinearSVC\":\n",
    "        model = LinearSVC(featuresCol='features', labelCol=variavel_resposta, **best_params)\n",
    "    elif winning_model_name == \"NaiveBayes\":\n",
    "        model = NaiveBayes(featuresCol='features', labelCol=variavel_resposta, **best_params)\n",
    "    elif winning_model_name == \"MultilayerPerceptronClassifier\":\n",
    "        model = MultilayerPerceptronClassifier(featuresCol='features', labelCol=variavel_resposta, \n",
    "                                               layers=[len(features_cols), 5, 2], seed=seed, **best_params)\n",
    "    elif winning_model_name == \"FMClassifier\":\n",
    "        model = FMClassifier(featuresCol='features', labelCol=variavel_resposta, **best_params)\n",
    "    elif winning_model_name == \"LightGBMClassifier\":\n",
    "        model = LightGBMClassifier(featuresCol='features', labelCol=variavel_resposta, predictionCol=\"prediction\", seed=seed, **best_params)\n",
    "    elif winning_model_name == \"OneVsRest\":\n",
    "        model = OneVsRest(classifier=LogisticRegression(featuresCol='features', labelCol=variavel_resposta, **best_params))\n",
    "    \n",
    "    # Avaliar o modelo otimizado\n",
    "    print(f\"Avaliando {winning_model_name} otimizado...\")\n",
    "    optimized_metrics, optimized_predictions = evaluate_model(model, train_df, test_df, variavel_resposta)\n",
    "    print(f\"{winning_model_name} (Otimizado) - Métricas: {optimized_metrics}\")\n",
    "    \n",
    "    # Salvar o modelo e os hiperparâmetros\n",
    "    save_model_and_params(model, best_params, winning_model_name, save_dir)\n",
    "    \n",
    "    # Adicionar o modelo otimizado aos resultados\n",
    "    optimized_results = {\n",
    "        \"Modelo\": f\"{winning_model_name} (Otimizado)\",\n",
    "        **optimized_metrics,\n",
    "        \"Hiperparâmetros\": best_params\n",
    "    }\n",
    "    results.append((\"Modelo Otimizado\", optimized_results))\n",
    "    \n",
    "    # Converter resultados para DataFrame\n",
    "    result_df = pd.DataFrame([{\n",
    "        \"Modelo\": model_name,\n",
    "        **metrics\n",
    "    } for model_name, metrics in results])\n",
    "    \n",
    "    # Exibir o DataFrame de resultados\n",
    "    print(\"Resultados dos modelos:\")\n",
    "    print(result_df)\n",
    "    \n",
    "    # Plotar gráficos apenas para o modelo vencedor otimizado\n",
    "    if \"ConfusionMatrix\" in optimized_metrics:\n",
    "        plot_confusion_matrix(optimized_metrics[\"ConfusionMatrix\"])\n",
    "    if \"AUC\" in optimized_metrics:\n",
    "        plot_roc_curve(optimized_predictions, variavel_resposta)\n",
    "        plot_pr_curve(optimized_predictions, variavel_resposta)\n",
    "        plot_probability_distribution(optimized_predictions, variavel_resposta)\n",
    "        plot_calibration_curve(optimized_predictions, variavel_resposta)\n",
    "        plot_ks_curve(optimized_predictions, variavel_resposta)\n",
    "        plot_lift_curve(optimized_predictions, variavel_resposta)\n",
    "    if winning_model_name in [\"RandomForestClassifier\", \"GBTClassifier\"]:\n",
    "        plot_feature_importance(model, features_cols)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    data = [(1.0, 2.0, 3.0, 0), (4.0, 5.0, 6.0, 1), (7.0, 8.0, 9.0, 0), (10.0, 11.0, 12.0, 1)]\n",
    "    columns = [\"feature1\", \"feature2\", \"feature3\", \"target\"]\n",
    "    spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    features_cols = [\"feature1\", \"feature2\", \"feature3\"]\n",
    "    variavel_resposta = \"target\"\n",
    "    metricas_disponiveis = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\", \"AUC\", \"KS\"]\n",
    "    metrica_vencedora = \"accuracy\"\n",
    "    seed = 42\n",
    "    save_dir = \"/dbfs/mnt/your_directory\"\n",
    "    \n",
    "    result_df = main(df, features_cols, variavel_resposta, metricas_disponiveis, metrica_vencedora, seed, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
