from pyspark.sql import DataFrame
from pyspark.sql.functions import col, regexp_replace, when
from pyspark.sql.types import DoubleType

def convert_numeric_strings_with_log(df: DataFrame, sample_size: int = 1000) -> DataFrame:
    """
    Converte colunas string que contêm números com separadores de milhares para DoubleType,
    com logging detalhado do processo.
    
    Args:
        df: DataFrame do Spark
        sample_size: Número de linhas para amostrar na verificação
    
    Returns:
        DataFrame com colunas numéricas convertidas
    """
    print("\nIniciando análise de conversão de colunas numéricas...")
    
    # Identificar colunas string candidatas
    string_cols = [field.name for field in df.schema.fields 
                  if str(field.dataType) == 'StringType()']
    print(f"\nTotal de colunas string analisadas: {len(string_cols)}")
    
    cols_to_convert = []
    conversion_details = []
    
    for col_name in string_cols:
        # Amostrar dados para verificar padrões numéricos
        sample = df.select(col_name).filter(col(col_name).isNotNull()).limit(sample_size).collect()
        
        numeric_count = 0
        total_count = 0
        example_values = set()
        
        for row in sample:
            value = row[0]
            if value is not None:
                total_count += 1
                # Verificar padrões numéricos com separadores
                cleaned_value = value.replace('.', '').replace(',', '').replace('-', '')
                if cleaned_value.isdigit() and ('.' in value or ',' in value):
                    numeric_count += 1
                    if len(example_values) < 3:  # Guardar até 3 exemplos
                        example_values.add(value)
        
        # Determinar se deve converter
        if total_count > 0 and (numeric_count / total_count) > 0.8:
            cols_to_convert.append(col_name)
            conversion_details.append({
                'coluna': col_name,
                'total': total_count,
                'numericos': numeric_count,
                '%_numericos': (numeric_count / total_count) * 100,
                'exemplos': list(example_values)
            })
    
    # Log de colunas detectadas
    print(f"\nColunas identificadas para conversão ({len(cols_to_convert)}):")
    for detail in conversion_details:
        print(f" - {detail['coluna']}:")
        print(f"   • % valores numéricos: {detail['%_numericos']:.2f}%")
        print(f"   • Exemplos: {', '.join(detail['exemplos'])}")
    
    # Converter as colunas identificadas
    converted_count = 0
    for col_name in cols_to_convert:
        try:
            df = df.withColumn(
                col_name,
                # Trata valores negativos
                when(col(col_name).startswith('-'), 
                     -1 * regexp_replace(regexp_replace(col(col_name), '^-', ''), '\.', '').cast('double'))
                .otherwise(
                    regexp_replace(regexp_replace(col(col_name), '\.', ''), ',', '.').cast('double'))
            )
            converted_count += 1
        except Exception as e:
            print(f"Erro ao converter {col_name}: {str(e)}")
            continue
    
    # Resumo final
    print("\nResumo da conversão:")
    print(f"• Colunas analisadas: {len(string_cols)}")
    print(f"• Colunas detectadas como numéricas: {len(cols_to_convert)}")
    print(f"• Colunas convertidas com sucesso: {converted_count}")
    if len(cols_to_convert) > converted_count:
        print(f"• Colunas com erro na conversão: {len(cols_to_convert) - converted_count}")
    
    return df

# Exemplo de uso completo:
if __name__ == "__main__":
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder.appName("ConversionExample").getOrCreate()
    
    # Criar DataFrame de exemplo
    data = [
        ("1.234,56", "100,50", "texto"),
        ("-2.345,67", "200,75", "123"),
        ("3.000,00", "ABC", "1.000"),
        ("1.500,99", "300,25", "outro texto")
    ]
    df = spark.createDataFrame(data, ["valor1", "valor2", "valor3"])
    
    print("DataFrame original:")
    df.show()
    
    # Aplicar a função
    df_converted = convert_numeric_strings_with_log(df)
    
    print("\nDataFrame após conversão:")
    df_converted.show()
    df_converted.printSchema()