from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline

# 1. Inicializar Spark Session (caso não exista)
spark = SparkSession.builder \
    .appName("OptimizedOneHotEncoding") \
    .getOrCreate()

# 2. Função principal com logging
def optimized_onehot_encode_with_log(df: DataFrame, 
                                   categorical_cols: list,
                                   min_freq: float = 0.01,
                                   max_categories: int = 50,
                                   drop_original: bool = True,
                                   drop_last: bool = True) -> DataFrame:
    """
    OneHot Encoding com redução prévia de cardinalidade e logging informativo.
    
    Args:
        df: DataFrame Spark
        categorical_cols: Lista de colunas categóricas
        min_freq: Frequência mínima para manter uma categoria (0-1)
        max_categories: Número máximo de categorias por coluna
        drop_original: Se True, remove colunas originais
        drop_last: Se True, evita multicolinearidade
    
    Returns:
        DataFrame com colunas codificadas otimizadas
    """
    # 1. Pré-processamento: Redução de categorias com logging
    processed_df = df
    for col in categorical_cols:
        # Calcular estatísticas iniciais
        initial_count = df.select(col).distinct().count()
        
        # Calcular frequência de cada categoria
        freq_df = (df.groupBy(col)
                   .agg(F.count("*").alias("count"))
        
        # Calcular frequência relativa
        total = df.count()
        freq_df = freq_df.withColumn("freq", F.col("count")/total)
        
        # Ordenar por frequência
        freq_df = freq_df.orderBy(F.desc("count"))
        
        # Selecionar top categorias
        top_categories_df = freq_df.filter(F.col("freq") >= min_freq).limit(max_categories)
        top_categories = [row[col] for row in top_categories_df.collect()]
        
        # Calcular estatísticas após filtro
        final_count = len(top_categories)
        kept_percentage = (top_categories_df.agg(F.sum("freq")).collect()[0][0] or 0) * 100
        
        # Log de redução
        print(f"\nColuna '{col}':")
        print(f"  - Categorias originais: {initial_count}")
        print(f"  - Categorias mantidas: {final_count}")
        print(f"  - % dos dados mantidos: {kept_percentage:.2f}%")
        if initial_count > final_count:
            print(f"  - Redução de {initial_count - final_count} categorias")
        
        # Criar nova coluna com categorias reduzidas
        processed_df = processed_df.withColumn(
            f"{col}_reduced",
            F.when(F.col(col).isin(top_categories), F.col(col))
             .otherwise(F.lit("__OTHER__"))
        )
    
    # 2. Aplicar OneHot Encoding
    reduced_cols = [f"{col}_reduced" for col in categorical_cols]
    
    indexers = [
        StringIndexer(inputCol=col, outputCol=f"{col}_idx", handleInvalid="keep")
        for col in reduced_cols
    ]
    
    encoder = OneHotEncoder(
        inputCols=[f"{col}_idx" for col in reduced_cols],
        outputCols=[f"{col}_encoded" for col in reduced_cols],
        dropLast=drop_last
    )
    
    pipeline = Pipeline(stages=indexers + [encoder])
    model = pipeline.fit(processed_df)
    encoded_df = model.transform(processed_df)
    
    # 3. Pós-processamento
    original_cols = [c for c in df.columns if c not in categorical_cols]
    
    # Renomear colunas codificadas
    for orig_col in categorical_cols:
        encoded_col = f"{orig_col}_reduced_encoded"
        encoded_df = encoded_df.withColumnRenamed(encoded_col, orig_col)
    
    # Selecionar colunas finais
    final_cols = original_cols + categorical_cols
    if not drop_original:
        final_cols += categorical_cols
    
    print("\nProcesso concluído com sucesso!")
    return encoded_df.select(final_cols)

# 3. Exemplo de Uso com Dados Simulados
if __name__ == "__main__":
    print("="*50)
    print("EXEMPLO PRÁTICO DE USO")
    print("="*50)
    
    # Criar DataFrame de exemplo
    data = [
        ("produto_A", "SP", "tipo_1"),
        ("produto_B", "RJ", "tipo_2"),
        ("produto_C", "MG", "tipo_1"),
        ("produto_A", "SP", "tipo_3"),
        ("produto_D", "RJ", "tipo_2"),
        ("produto_A", "RS", "tipo_1"),
        ("produto_B", "SP", "tipo_4"),
        ("produto_E", "SP", "tipo_1"),  # Categoria rara
        ("produto_F", "RJ", "tipo_5"),  # Categoria rara
        ("produto_A", "MG", "tipo_1")
    ]
    
    df_exemplo = spark.createDataFrame(data, ["produto", "estado", "tipo_cliente"])
    
    print("\nDataFrame Original:")
    df_exemplo.show()
    
    # Lista de colunas categóricas para transformar
    cat_cols = ["produto", "estado", "tipo_cliente"]
    
    print("\nIniciando processo de OneHot Encoding otimizado...")
    
    # Aplicar a função com parâmetros restritivos para demonstração
    df_encoded = optimized_onehot_encode_with_log(
        df=df_exemplo,
        categorical_cols=cat_cols,
        min_freq=0.2,        # 20% de frequência mínima (para demo com pouco dados)
        max_categories=2,    # Apenas 2 categorias por coluna (para demo)
        drop_last=True
    )
    
    print("\nDataFrame após OneHot Encoding:")
    df_encoded.show()
    
    print("\nEsquema final:")
    df_encoded.printSchema()